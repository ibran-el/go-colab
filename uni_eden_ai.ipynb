{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cIAFM6lu9VyVBVwujTyeAC95SAGkAOs7",
      "authorship_tag": "ABX9TyNPOWZUYOfRvJP0dYdijacB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibran-el/go-colab/blob/main/uni_eden_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Codes Below illustrates how to use langchain to fine-tune a large language model on specific data (custom data) from a document, pdf, textfile, or a markdown. in this case, we are using Eden AI, you can use Open AI or other many models supported by langchain. some lines of code vary depending on the LLM provider. you might wanna consider checking out lang chains documentation."
      ],
      "metadata": {
        "id": "OAxHAC-MB8jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsL3qMbuucS8",
        "outputId": "555e2e7b-3390-4a4f-c0b1-dbca388bb3ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCfQzNTBuJL7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "REqvXwZXwbmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "P8BQHXO_whjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask Streamlit"
      ],
      "metadata": {
        "id": "zNYxuRJvw4TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu faiss-gpu"
      ],
      "metadata": {
        "id": "TKx8b-Rpw_DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "1LTcNQr74ozC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT IMPORTS"
      ],
      "metadata": {
        "id": "Bf47CzjU9Py8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import docx\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# imports below are langchain specific import\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# the imports down below are EDEN AI related imports\n",
        "from langchain_community.llms import EdenAI\n",
        "from langchain_community.embeddings.edenai import EdenAiEmbeddings\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['EDENAI_API_KEY'] = userdata.get('EDEN_KEY')\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "GzZUduTaxh8n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FILE READING FUNCTIONS"
      ],
      "metadata": {
        "id": "Z5am2pQ39SA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# files reading functions\n",
        "def for_pdf(dir_path):\n",
        "    with open(dir_path, 'rb') as pfile:\n",
        "        pdf_r = PdfReader(pfile)\n",
        "        text = \"\"\n",
        "        for page in range(len(pdf_r.pages)):\n",
        "            text+=pdf_r.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "def for_doc(dir_path):\n",
        "    with open(dir_path, 'r'):\n",
        "        doc_r = docx.Document(dir_path)\n",
        "        text = \"\"\n",
        "        for par in doc_r.paragraphs:\n",
        "            text += par.text + \"\\n\"\n",
        "        return text\n",
        "\n",
        "def for_text(dir_path):\n",
        "    with open(dir_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "\n",
        "#general document reading function\n",
        "def readFilez(directory):\n",
        "    combined_txt = \"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith('.txt'):\n",
        "            combined_txt += for_text(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            combined_txt += for_doc(file_path)\n",
        "        elif filename.endswith('.pdf'):\n",
        "            combined_txt += for_pdf(file_path)\n",
        "    return combined_txt"
      ],
      "metadata": {
        "id": "ik6arU-34rSP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORK CODES"
      ],
      "metadata": {
        "id": "Qg-kdyBN-KTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "divide text into chunks that that will be sent to memory for processing since we cannot load the whole document at once."
      ],
      "metadata": {
        "id": "JICKbMoQAfcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/edenAI/edentrain/data/'\n",
        "text = readFilez(data_dir)\n",
        "\n",
        "# split text into chunks for easy processing and memory management\n",
        "char_txt_splitter = CharacterTextSplitter(\n",
        "    separator='\\n', chunk_size=1000, chunk_overlap=200, length_function=len)\n",
        "\n",
        "text_chunks = char_txt_splitter.split_text(text)\n"
      ],
      "metadata": {
        "id": "VC861KW0-N4Z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create embeddings vectors"
      ],
      "metadata": {
        "id": "px2pxPTFAsCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = EdenAiEmbeddings(provider='openai')\n",
        "\n",
        "docsearch = FAISS.from_texts(text_chunks,embeddings)"
      ],
      "metadata": {
        "id": "ZdhyIdwzAybZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load an LLM"
      ],
      "metadata": {
        "id": "LNfC98yXB2Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = llm = EdenAI(edenai_api_key=os.getenv(\"EDENAI_API_KEY\"), provider=\"openai\", temperature=0.3, max_tokens=250)\n",
        "chain = load_qa_chain(llm, chain_type='stuff')"
      ],
      "metadata": {
        "id": "-Is-CHSVC2Nh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "querying the model, and 'chain' the response"
      ],
      "metadata": {
        "id": "XrxYirWIDYzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"what does sz003 mean\"\"\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "\n",
        "response = chain.run(input_documents=docs, question=query)\n",
        "\n",
        "print(\" \")\n",
        "print(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kg6EPIUDbkk",
        "outputId": "b0d723fd-872f-4f26-da36-9efd8f8385fb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "what does sz003 mean\n",
            " SZ003 refers to the code for the Bachelor of IT Application & Management program at the State University of Zanzibar (SUZA) in Zanzibar.\n"
          ]
        }
      ]
    }
  ]
}